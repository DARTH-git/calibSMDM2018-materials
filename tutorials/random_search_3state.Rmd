---
title: "Random Search Calibration: A Cancer Example"
author: "DARTH Workgroup"
date: "October 9, 2018"
output:
    html_document:
        toc: true
        depth: 3
        number_sections: true
        theme: united
        toc_float: true
        highlight: tango
---



The "Cancer Relative Survival Model" represents transitions between three states: No Evidence of Disease (NED), cAncer metastasis (Mets), and Cancer Death.

<br>

<figure>
  <img src="../images/crs_state_diagram.jpeg" alt="drawing" width="500"/>
  <figcaption>CRS State Diagram (Alarid-Escudero, F, et al. MDM 2018)</figcaption>
</figure>

<br>

We need to calibrate the two transition probabilities in the figure above - `p.Mets`, `p.DieMets` - because these transition probabilities are unobservable (at least in this example).

To calibrate `p.Mets` and `p.DieMets`, we will fit the model to observed survival data. To do this, we'll sample the two parameters randomly, and then measure how well the model-predicted survival matches the observed survival.

We'll use the following packages:

```{r}
# calibration functionality
library(lhs)

# visualization
library(plotrix)
library(psych)
```


# Targets

The hypothetical calibration target is the proportion of people that survive over time. 

```{r}
load("data/CRSTargets.RData")
```


Let's see what the survival looks like.

```{r}
plotrix::plotCI(x = CRS.targets$Surv$Time, y = CRS.targets$Surv$value, 
                ui = CRS.targets$Surv$ub,
                li = CRS.targets$Surv$lb,
                ylim = c(0, 1), 
                xlab = "Time", ylab = "Pr Survive")
```

Note that the estimate of survival has some uncertainty around it, indicated by the error bounds.

If you had another target (stored in `CRS.targets$Target2`), you could plot it like this:

```
plotrix::plotCI(x = CRS.targets$Target2$Time, y = CRS.targets$Target2$value,
                ui = CRS.targets$Target2$ub,
                li = CRS.targets$Target2$lb,
                ylim = c(0, 1),
                xlab = "Time", ylab = "Target 2")
```


# Model

The inputs to the function are parameters to be estimated through calibration. The model outputs correspond to the target data.

This creates the function `markov_crs()`. The function has one argument, `v.params`, which is a vector of parameters. In this case, `v.params` is a named vector with elements `p.Mets` and `p.DieMets`.


```{r}
source("scripts/markov_crs.R") 
```


Let's check that the model runs. We arbitrarily set some parameter values in `v.parms.test`, then run the model. 

```{r}
v.params.test <- c(p.Mets = 0.1, p.DieMets = 0.2)
test_results <- markov_crs(v.params.test)
```

Within the returned model object (a list), the predicted survival is in a vector called `Surv`:
```{r}
str(test_results)
head(test_results$Surv)
plot(test_results$Surv)
```

It works!

But let's compare the outputs to the targets.

```{r}
plotrix::plotCI(x = CRS.targets$Surv$Time, y = CRS.targets$Surv$value, 
                ui = CRS.targets$Surv$ub,
                li = CRS.targets$Surv$lb,
                ylim = c(0, 1), 
                xlab = "Time", ylab = "Pr Survive")
points(test_results$Surv,
       col = "green", pch = 20)
legend("topright", legend = c("Targets", "Outputs"),
       col = c("black", "green"),
       pch = c(1, 20))
```

That doesn't look right! We need to *calibrate* the unobservable parameters so that the results match the observed data.

# Specify Calibration Inputs

For random search with latin hypercube sampling, we need to specify:

1. The number of samples
2. The range of each parameter


First, when sampling random numbers in `R`, setting a seed allows you to obtain the same sample each time.

```{r}
set.seed(1010)
```

Let's start with 1000 samples. We're calibrating 2 parameters, `p.Mets` and `p.DieMets`.

```{r}
param.names <- c("p.Mets","p.DieMets")
n.param <- length(param.names)
n.samp <- 1000
```

The boundaries on the search space are as follows:
```{r}
# lower bound
lb <- c(p.Mets = 0.04, p.DieMets = 0.04) 

# upper bound
ub <- c(p.Mets = 0.16, p.DieMets = 0.16)
```

Now, we sample. Right away, we rename the columns of `m.lhs.unit` so we can keep track of which column is which.

```{r}
# Sample unit Latin Hypercube
m.lhs.unit <- lhs::randomLHS(n.samp, n.param)
colnames(m.lhs.unit) <- param.names
head(m.lhs.unit)
```

Note that this does not yet incorporate the ranges we specified. 

```{r fig.width = 6}
plot(x = m.lhs.unit[, param.names[1]],
     y = m.lhs.unit[, param.names[2]],
     xlab = param.names[1],
     ylab = param.names[2])
```

We have to rescale to the ranges we specified. (Note: we can start with a LHS sample and then transform it to *any* distribution - in this case, we're rescaling it to a uniform distribution)

```{r}
# Rescale to min/max of each parameter
m.param.samp <- matrix(nrow=n.samp,ncol=n.param)
colnames(m.param.samp) <- param.names
for (i in 1:n.param){
  m.param.samp[,i] <- qunif(m.lhs.unit[,i],
                           min = lb[i],
                           max = ub[i])
}
```

Now, the parameters are within the range we specified.

```{r}
# view resulting parameter set samples
psych::pairs.panels(m.param.samp)
```


# Run the Calibration

We have `r n.samp` parameter sets, so we'll have `r n.samp` entries in the goodness-of-fit vector. We're currently using a single target, so we'll have one column.

```{r}
# initialize goodness-of-fit vector
m.GOF <- rep(0, n.samp)
```

Now, we run the model for each set of input values, and determine how well the model results fit the target data.

## Goodness-of-fit

In this case, the goodness of fit function is the likelihood of observing the target data, given the model results. We assume a normal likelihood for this example. In many cases, it's more numerically stable to use the log-likelihood `log = TRUE` and sum the individual likelihoods together.

```{r}
gof_norm_loglike <- function(target_mean, target_sd, model_output){
  sum(dnorm(x = target_mean,
            mean = model_output,
            sd = target_sd,
            log = TRUE))
}
```

Loop through sampled sets of input values

```{r}
for (j in 1:n.samp){
  
  ###  Run model for a given parameter set  ###
  model.res = markov_crs(v.params = m.param.samp[j, ])
  
  ###  Calculate goodness-of-fit of model outputs to targets  ###
  # log likelihood of the model output given the targets
  m.GOF[j] = gof_norm_loglike(model_output = model.res$Surv,
                              target_mean = CRS.targets$Surv$value,
                              target_sd = CRS.targets$Surv$se)
}
```

## Best-fitting parameters

```{r}
# Arrange parameter sets in order of fit
m.calib.res <- cbind(m.param.samp,m.GOF)
m.calib.res <- m.calib.res[order(-m.calib.res[,"m.GOF"]),]

# Examine the top 10 best-fitting sets
m.calib.res[1:10,]

# Plot the top 100 (top 10%)
plot(x = m.calib.res[1:100,1], y = m.calib.res[1:100,2],
     xlim=c(lb[1],ub[1]),ylim=c(lb[2],ub[2]),
     xlab = colnames(m.calib.res)[1],ylab = colnames(m.calib.res)[2])
```

How does the model look with the best-fitting set?

```{r}

best_fit_params <- c(m.calib.res[1, c("p.Mets",  "p.DieMets")])

best_fit_model <- markov_crs(best_fit_params)

plotrix::plotCI(x = CRS.targets$Surv$Time, y = CRS.targets$Surv$value, 
                ui = CRS.targets$Surv$ub,
                li = CRS.targets$Surv$lb,
                ylim = c(0, 1), 
                xlab = "Time", ylab = "Pr Survive")

points(x = names(best_fit_model$Surv), y = best_fit_model$Surv,
       col = "green",
       pch = 20)
legend("topright", legend = c("Targets", "Outputs"),
       col = c("black", "green"),
       pch = c(1, 20))
```

## Other likelihoods

We used a normal likelihood as a goodness-of-fit measure above, but there are many other options. For example, we could use a weighted sum of squared errors.

```
gof_wsse <- function(model_output, target_mean, target_se){
    w = 1/(target_se^2)
    gof <-  (-1) * sum(w*(model_output - target_mean)^2) 
    return(gof)
}
```


## Multiple Targets

There is nothing restricting us to a single target, either. If we do use multiple targets, we can combine the multiple goodness-of-fit measures into a single GOF, possibly using weights to indicate the importance of each target.

If we had 2 targets, we would define a GOF *matrix* with two columns:

```
# initialize goodness-of-fit matrix
m.GOF <- matrix(0, nrow = n.samp, ncol = 2)

for (j in 1:n.samp){
  
  ###  Run model for a given parameter set  ###
  model.res = markov_crs(v.params = m.param.samp[j, ])
  
  ###  Target 1  ###
  m.GOF[j, 1] = gof_norm_loglike(model_output = model.res$Output1,
                              target_mean = CRS.targets$Target1$value,
                              target_sd = CRS.targets$Target1$se)
  
  ### Target 2 ###
  m.GOF[j, 2] = gof_norm_loglike(model_output = model.res$Output2,
                              target_mean = CRS.targets$Target2$value,
                              target_sd = CRS.targets$Target2$se)
}
```

Then, we would combine the GoFs into a single, multiplying by a `v.weights`, a vector of weights (here, equal weights).
```
# can give different targets different weights
v.weights <- matrix(1, nrow = n.target, ncol = 1)

# matrix multiplication to calculate weight sum of each GOF matrix row
v.GOF.overall <- c(m.GOF%*%v.weights)

# Store in GOF matrix with column name "Overall"
m.GOF <- cbind(m.GOF,Overall_fit=v.GOF.overall)
```

# Return to Main Page

<a href="https://caleb-easterly.github.io/calibSMDM2018-materials/">Back to main page</a>